{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "import wandb\n",
    "from termcolor import cprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.datasets import ThingsMEGDataset\n",
    "from src.models import BasicConvClassifier\n",
    "from src.utils import set_seed\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 128\n",
    "epochs= 20\n",
    "lr= 0.0625\n",
    "\n",
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = \"cuda\" if cuda else \"cpu\"\n",
    "num_workers= 4\n",
    "seed= 1234\n",
    "use_wandb= False\n",
    "\n",
    "data_dir= \"data\"\n",
    "\n",
    "set_seed(seed)\n",
    "logdir = \"\"\n",
    "\n",
    "loader_args = {\"batch_size\": batch_size, \"num_workers\": num_workers}\n",
    "\n",
    "train_set = ThingsMEGDataset(\"train\", data_dir)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, **loader_args)\n",
    "val_set = ThingsMEGDataset(\"val\", data_dir)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, shuffle=False, **loader_args)\n",
    "test_set = ThingsMEGDataset(\"test\", data_dir)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, shuffle=False, batch_size=batch_size, num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops.layers.torch import Rearrange\n",
    "import numpy as np\n",
    "\n",
    "class BasicConvClassifier1(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        seq_len: int,\n",
    "        in_channels: int,\n",
    "        hid_dim: int = 128\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            ConvBlock(in_channels, hid_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            X ( b, c, t ): _description_\n",
    "        Returns:\n",
    "            X ( b, num_classes ): _description_\n",
    "        \"\"\"\n",
    "        X = self.blocks(X)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    \n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        out_dim,\n",
    "        input_window_samples=None,\n",
    "        n_filters_time=20,\n",
    "        filter_time_length=25,\n",
    "        n_filters_spat=40,\n",
    "        pool_time_length=75,\n",
    "        pool_time_stride=15,\n",
    "        final_conv_length=13,\n",
    "        pool_mode=\"mean\",\n",
    "        split_first_layer=True,\n",
    "        batch_norm=True,\n",
    "        batch_norm_alpha=0.1,\n",
    "        drop_prob=0.5,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.input_window_samples = input_window_samples\n",
    "        self.n_filters_time = n_filters_time\n",
    "        self.filter_time_length = filter_time_length\n",
    "        self.n_filters_spat = n_filters_spat\n",
    "        self.pool_time_length = pool_time_length\n",
    "        self.pool_time_stride = pool_time_stride\n",
    "        self.final_conv_length = final_conv_length\n",
    "        self.pool_mode = pool_mode\n",
    "        self.split_first_layer = split_first_layer\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_norm_alpha = batch_norm_alpha\n",
    "        self.drop_prob = drop_prob\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_classes = 1854\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, self.n_filters_time, (self.filter_time_length, 1), stride=1)\n",
    "        self.conv2 = nn.Conv2d(self.n_filters_time,self.n_filters_spat,(1,self.in_dim), stride=1,bias=not self.batch_norm)\n",
    "        self.norm3 = nn.BatchNorm2d(self.n_filters_spat, momentum=self.batch_norm_alpha, affine=True, track_running_stats=True)\n",
    "        self.avgp4 = nn.AvgPool2d(kernel_size=(self.pool_time_length, 1), stride=(self.pool_time_stride, 1), padding=0)\n",
    "        self.drop5 = nn.Dropout(p=self.drop_prob, inplace=False)\n",
    "        self.conv6 = nn.Conv2d(self.n_filters_spat, self.n_classes, (self.final_conv_length, 1), stride=(1, 1), bias=True)\n",
    "        self.soft7 = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def ensure4d(self, x):\n",
    "        while len(x.shape) < 4:\n",
    "            x = x.unsqueeze(-1)\n",
    "        return x\n",
    "    \n",
    "    def square(self,x):\n",
    "        return x * x\n",
    "\n",
    "    def safe_log(self,x, eps=1e-6):\n",
    "        return torch.log(torch.clamp(x, min=eps))\n",
    "\n",
    "    def transpose_time_to_spat(self,x):\n",
    "        return x.permute(0, 3, 2, 1)\n",
    "\n",
    "    def np_to_th(self,\n",
    "        X, requires_grad=False, dtype=None, pin_memory=False, **tensor_kwargs\n",
    "    ):\n",
    "        if not hasattr(X, \"__len__\"):\n",
    "            X = [X]\n",
    "        X = np.asarray(X)\n",
    "        if dtype is not None:\n",
    "            X = X.astype(dtype)\n",
    "        X_tensor = torch.tensor(X, requires_grad=requires_grad, **tensor_kwargs)\n",
    "        if pin_memory:\n",
    "            X_tensor = X_tensor.pin_memory()\n",
    "        return X_tensor\n",
    "\n",
    "    def squeeze_final_output(self,x):\n",
    "        assert x.size()[3] == 1\n",
    "        x = x[:, :, :, 0]\n",
    "        if x.size()[2] == 1:\n",
    "            x = x[:, :, 0]\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.ensure4d(x)\n",
    "        x = x.permute(0, 3, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.square(x)\n",
    "        x = self.avgp4(x)\n",
    "        x = self.safe_log(x, eps=1e-6)\n",
    "        x = self.drop5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.soft7(x)\n",
    "        x = self.squeeze_final_output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops.layers.torch import Rearrange\n",
    "import numpy as np\n",
    "\n",
    "class BasicConvClassifier2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        seq_len: int,\n",
    "        in_channels: int,\n",
    "        hid_dim: int = 128\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            ConvBlock(in_channels, hid_dim),\n",
    "            ConvBlock(hid_dim, hid_dim),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            Rearrange(\"b d 1 -> b d\"),\n",
    "            nn.Linear(hid_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            X ( b, c, t ): _description_\n",
    "        Returns:\n",
    "            X ( b, num_classes ): _description_\n",
    "        \"\"\"\n",
    "        X = self.blocks(X)\n",
    "\n",
    "        return self.head(X)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        out_dim,\n",
    "        kernel_size: int = 3,\n",
    "        p_drop: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.conv0 = nn.Conv1d(in_dim, out_dim, kernel_size, padding=\"same\")\n",
    "        self.conv1 = nn.Conv1d(out_dim, out_dim, kernel_size, padding=\"same\")\n",
    "        # self.conv2 = nn.Conv1d(out_dim, out_dim, kernel_size) # , padding=\"same\")\n",
    "        \n",
    "        self.batchnorm0 = nn.BatchNorm1d(num_features=out_dim)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_features=out_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        if self.in_dim == self.out_dim:\n",
    "            X = self.conv0(X) + X  # skip connection\n",
    "        else:\n",
    "            X = self.conv0(X)\n",
    "\n",
    "        X = F.gelu(self.batchnorm0(X))\n",
    "\n",
    "        X = self.conv1(X) + X  # skip connection\n",
    "        X = F.gelu(self.batchnorm1(X))\n",
    "\n",
    "        # X = self.conv2(X)\n",
    "        # X = F.glu(X, dim=-2)\n",
    "\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 514/514 [02:27<00:00,  3.49it/s]\n",
      "Validation: 100%|██████████| 129/129 [00:04<00:00, 29.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | train loss: 7.890 | train acc: 0.007 | val loss: 7.516 | val acc: 0.011\n",
      "\u001b[36mNew best.\u001b[0m\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 514/514 [02:28<00:00,  3.45it/s]\n",
      "Validation: 100%|██████████| 129/129 [00:03<00:00, 33.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | train loss: 7.434 | train acc: 0.019 | val loss: 7.496 | val acc: 0.015\n",
      "\u001b[36mNew best.\u001b[0m\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 514/514 [02:28<00:00,  3.46it/s]\n",
      "Validation: 100%|██████████| 129/129 [00:03<00:00, 32.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | train loss: 7.262 | train acc: 0.033 | val loss: 7.483 | val acc: 0.022\n",
      "\u001b[36mNew best.\u001b[0m\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 514/514 [02:28<00:00,  3.46it/s]\n",
      "Validation: 100%|██████████| 129/129 [00:03<00:00, 32.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | train loss: 7.040 | train acc: 0.058 | val loss: 7.512 | val acc: 0.021\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 514/514 [02:28<00:00,  3.46it/s]\n",
      "Validation: 100%|██████████| 129/129 [00:03<00:00, 33.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | train loss: 6.823 | train acc: 0.083 | val loss: 7.579 | val acc: 0.022\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 514/514 [02:28<00:00,  3.47it/s]\n",
      "Validation: 100%|██████████| 129/129 [00:03<00:00, 33.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | train loss: 6.637 | train acc: 0.109 | val loss: 7.618 | val acc: 0.022\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 514/514 [02:28<00:00,  3.45it/s]\n",
      "Validation: 100%|██████████| 129/129 [00:03<00:00, 33.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | train loss: 6.469 | train acc: 0.135 | val loss: 7.698 | val acc: 0.022\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 514/514 [02:29<00:00,  3.44it/s]\n",
      "Validation: 100%|██████████| 129/129 [00:03<00:00, 33.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | train loss: 6.320 | train acc: 0.157 | val loss: 7.742 | val acc: 0.023\n",
      "\u001b[36mNew best.\u001b[0m\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 514/514 [02:28<00:00,  3.46it/s]\n",
      "Validation: 100%|██████████| 129/129 [00:03<00:00, 33.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | train loss: 6.200 | train acc: 0.175 | val loss: 7.757 | val acc: 0.021\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 514/514 [02:28<00:00,  3.45it/s]\n",
      "Validation: 100%|██████████| 129/129 [00:03<00:00, 33.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 | train loss: 6.099 | train acc: 0.191 | val loss: 7.836 | val acc: 0.021\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 514/514 [02:28<00:00,  3.46it/s]\n",
      "Validation: 100%|██████████| 129/129 [00:03<00:00, 33.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 | train loss: 6.001 | train acc: 0.208 | val loss: 7.858 | val acc: 0.022\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 514/514 [02:29<00:00,  3.45it/s]\n",
      "Validation: 100%|██████████| 129/129 [00:03<00:00, 33.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 | train loss: 5.918 | train acc: 0.222 | val loss: 7.914 | val acc: 0.023\n",
      "\u001b[36mNew best.\u001b[0m\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 514/514 [02:28<00:00,  3.46it/s]\n",
      "Validation: 100%|██████████| 129/129 [00:03<00:00, 33.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 | train loss: 5.847 | train acc: 0.234 | val loss: 7.930 | val acc: 0.024\n",
      "\u001b[36mNew best.\u001b[0m\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 514/514 [02:29<00:00,  3.45it/s]\n",
      "Validation: 100%|██████████| 129/129 [00:03<00:00, 33.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 | train loss: 5.781 | train acc: 0.246 | val loss: 7.961 | val acc: 0.023\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  71%|███████▏  | 367/514 [01:46<00:42,  3.43it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     41\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 43\u001b[0m     acc \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     train_acc\u001b[38;5;241m.\u001b[39mappend(acc\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     46\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\ummii\\anaconda3\\envs\\dlbasics\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ummii\\anaconda3\\envs\\dlbasics\\lib\\site-packages\\torchmetrics\\metric.py:236\u001b[0m, in \u001b[0;36mMetric.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_reduce_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[1;32mc:\\Users\\ummii\\anaconda3\\envs\\dlbasics\\lib\\site-packages\\torchmetrics\\metric.py:302\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[1;32m--> 302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    303\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m    305\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ummii\\anaconda3\\envs\\dlbasics\\lib\\site-packages\\torchmetrics\\metric.py:390\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m         update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[1;32mc:\\Users\\ummii\\anaconda3\\envs\\dlbasics\\lib\\site-packages\\torchmetrics\\classification\\stat_scores.py:315\u001b[0m, in \u001b[0;36mMulticlassStatScores.update\u001b[1;34m(self, preds, target)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_args:\n\u001b[1;32m--> 315\u001b[0m     \u001b[43m_multiclass_stat_scores_tensor_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_format(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k)\n\u001b[0;32m    319\u001b[0m tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_update(\n\u001b[0;32m    320\u001b[0m     preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[0;32m    321\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ummii\\anaconda3\\envs\\dlbasics\\lib\\site-packages\\torchmetrics\\functional\\classification\\stat_scores.py:303\u001b[0m, in \u001b[0;36m_multiclass_stat_scores_tensor_validation\u001b[1;34m(preds, target, num_classes, multidim_average, ignore_index)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither `preds` and `target` both should have the (same) shape (N, ...), or `target` should be (N, ...)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and `preds` should be (N, C, ...).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m     )\n\u001b[1;32m--> 303\u001b[0m num_unique_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     check \u001b[38;5;241m=\u001b[39m num_unique_values \u001b[38;5;241m>\u001b[39m num_classes\n",
      "File \u001b[1;32mc:\\Users\\ummii\\anaconda3\\envs\\dlbasics\\lib\\site-packages\\torch\\_jit_internal.py:423\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ummii\\anaconda3\\envs\\dlbasics\\lib\\site-packages\\torch\\_jit_internal.py:423\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ummii\\anaconda3\\envs\\dlbasics\\lib\\site-packages\\torch\\functional.py:851\u001b[0m, in \u001b[0;36m_return_output\u001b[1;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[1;32m--> 851\u001b[0m output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_unique_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\ummii\\anaconda3\\envs\\dlbasics\\lib\\site-packages\\torch\\functional.py:765\u001b[0m, in \u001b[0;36m_unique_impl\u001b[1;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[0;32m    757\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39munique_dim(\n\u001b[0;32m    758\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    759\u001b[0m         dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    762\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[0;32m    763\u001b[0m     )\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 765\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unique2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    766\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size= 128\n",
    "epochs= 50\n",
    "lr= 0.0625*0.1\n",
    "print(device)\n",
    "# ------------------\n",
    "#       Model\n",
    "# ------------------\n",
    "model = BasicConvClassifier1(\n",
    "    train_set.num_classes, train_set.seq_len, train_set.num_channels\n",
    ").to(device)\n",
    "# model = BasicConvClassifier2(\n",
    "#     train_set.num_classes, train_set.seq_len, train_set.num_channels\n",
    "# ).to(device)\n",
    "# model = BasicConvClassifier1(\n",
    "#     train_set.num_classes, train_set.seq_len, train_set.num_channels\n",
    "# ).to(device)\n",
    "\n",
    "# ------------------\n",
    "#     Optimizer\n",
    "# ------------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# ------------------\n",
    "#   Start training\n",
    "# ------------------  \n",
    "max_val_acc = 0\n",
    "accuracy = Accuracy(\n",
    "    task=\"multiclass\", num_classes=train_set.num_classes, top_k=10\n",
    ").to(device)\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
    "    \n",
    "    model.train()\n",
    "    for X, y, subject_idxs in tqdm(train_loader, desc=\"Train\"):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = accuracy(y_pred, y)\n",
    "        train_acc.append(acc.item())\n",
    "\n",
    "    model.eval()\n",
    "    for X, y, subject_idxs in tqdm(val_loader, desc=\"Validation\"):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X)\n",
    "        \n",
    "        val_loss.append(F.cross_entropy(y_pred, y).item())\n",
    "        val_acc.append(accuracy(y_pred, y).item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | train loss: {np.mean(train_loss):.3f} | train acc: {np.mean(train_acc):.3f} | val loss: {np.mean(val_loss):.3f} | val acc: {np.mean(val_acc):.3f}\")\n",
    "    torch.save(model.state_dict(), os.path.join(logdir, \"model_last.pt\"))\n",
    "\n",
    "    if np.mean(val_acc) > max_val_acc:\n",
    "        cprint(\"New best.\", \"cyan\")\n",
    "        torch.save(model.state_dict(), os.path.join(logdir, \"model_best.pt\"))\n",
    "        max_val_acc = np.mean(val_acc)\n",
    "        \n",
    "\n",
    "# ----------------------------------\n",
    "#  Start evaluation with best model\n",
    "# ----------------------------------\n",
    "model.load_state_dict(torch.load(os.path.join(logdir, \"model_best.pt\"), map_location=device))\n",
    "\n",
    "preds = [] \n",
    "model.eval()\n",
    "for X, subject_idxs in tqdm(test_loader, desc=\"Validation\"):        \n",
    "    preds.append(model(X.to(device)).detach().cpu())\n",
    "    \n",
    "preds = torch.cat(preds, dim=0).numpy()\n",
    "np.save(os.path.join(logdir, \"submission\"), preds)\n",
    "cprint(f\"Submission {preds.shape} saved at {logdir}\", \"cyan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model_best.pt\"\n",
    "set_seed(seed)\n",
    "savedir = os.path.dirname(model_path)\n",
    "\n",
    "# ------------------\n",
    "#    Dataloader\n",
    "# ------------------    \n",
    "test_set = ThingsMEGDataset(\"test\", data_dir)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, shuffle=False, batch_size=batch_size, num_workers=num_workers\n",
    ")\n",
    "\n",
    "# ------------------\n",
    "#       Model\n",
    "# ------------------\n",
    "model = BasicConvClassifier(\n",
    "    test_set.num_classes, test_set.seq_len, test_set.num_channels\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "# ------------------\n",
    "#  Start evaluation\n",
    "# ------------------ \n",
    "preds = [] \n",
    "model.eval()\n",
    "for X, subject_idxs in tqdm(test_loader, desc=\"Validation\"):        \n",
    "    preds.append(model(X.to(device)).detach().cpu())\n",
    "    \n",
    "preds = torch.cat(preds, dim=0).numpy()\n",
    "np.save(os.path.join(savedir, \"submission\"), preds)\n",
    "cprint(f\"Submission {preds.shape} saved at {savedir}\", \"cyan\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlbasics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
